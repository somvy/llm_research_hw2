# Novel Ops: GRPO на Qwen2.5-1.5B-Instruct

### Задача
Я хотел поиграться в сторону faithfulness, чтобы модель опиралась на свои размышления а не память. Как полностью уйти от мемоизации? (при этом чтобы задача была подъемной для маленькой модели)? 
Ввести новые несложные операторы со смешными значками ⊕: например `a ⊕ b = 2*a + 3*b + 1`).
При этом ревард учитывает как корректность каждого шага цепочки, так и финальный ответ.

Три уровня сложности: 

- **easy** (1 оператор, глубина 1), 

- **medium** (2 оператора, глубина 2), 

- **hard** (3 оператора, глубина 3 + более сложные формулы).

Получается что-то типа такого промтпта:

```
Below are definitions of new mathematical operators.

Definitions:
a ⊜ b = a^2 + 1*b + 3
a ⊙ b = 4*a - 3*b

Compute step by step: 3 ⊙ (15 ⊜ 14)
```
Обучил это на лоре 64 ранг, альфа 64. Остальные гиперпараметры в train.py

wandb [report](https://wandb.ai/therem/huggingface/reports/GRPO-on-novel-operators-report--VmlldzoxNjA0ODQyMA?accessToken=2cf48vw5j0nglqptrysvzyw7yde66gyumgwvqfxxsfvqk1n3krhfhg6gvc5w82zx)

hf [model](https://huggingface.co/therem/novel_ops_grpo_lora)

![Результаты eval](eval_results.png)

## Анализ

**Easy.** На лёгких задачах GRPO-модель показывает рост best@1: с 0.80 до 0.98. Модель научилась стабильно решать простые выражения с первой попытки, std очень небольшое. При best@4 все три модели выходят на ~0.99, т.е. потолок задачи достигнут.

**Medium.** Здесь уже начались приколы. best@1 после GRPO чуть выше (0.67 vs 0.61), но best@4 у бейзлайна заметно лучше (0.88 vs 0.79). Это значит, что бейзлайн при достаточном количестве попыток находит правильный ответ чаще. 

**Hard.** На сложных задачах GRPO деградировал: best@1 упал с 0.27 до 0.15-0.16, best@4 с 0.55 до 0.27-0.31. Скорее всего, модель ревард хакнулась под лёгкие задач — научилась уверенно выписывать шаги, но потеряла способность к сложному ризонингу на длинных выражениях.

**Общий вывод.** GRPO прокачал формат и уверенность на простых задачах, но привёл к reward hacking: модель получала высокий ревард за правильный формат и простые шаги. 


## Faithfulness

Изначально хотел эту таску взять для faithfullness, давайте посмотрим что получилось. 
Нужно оценить насколько цепочки соответствуют вычислению, а не просто подогнаны под финальный ответ. Парсим шаги модели (`a ⊕ b = ... = result`) и проверяеем против ground truth вычисления оператора (у нас есть как раз граф вычислений по построению задачи)

Метрики:

**has_steps_rate** - доля сэмплов, где модель вообще выписала хотя бы один шаг

**step_accuracy** - средняя доля корректных шагов (среди тех сэмплов, где шаги есть)

**faithful_when_correct** - step_accuracy только для сэмплов с правильным финальным ответом


![Faithfulness](eval_faithfulness.png)

Результаты прикольные. Faithfulness у всех моделей довольно низкий, но после GRPO стало ещё хуже (??):

Base выписывает шаги только в ~50-75% случаев, но когда выписывает, step_accuracy ~15% (easy) -> ~7% (hard) - плохо, но хоть что-то.

Модели научились всегда выписывать шаги (has_steps  ~1.0), но step_accuracy упал до 0.7-0.9%(easy) -> 3-4%(medium/hard). Шаги по сути рандомные числа в правильном формате.

`faithful_when_correct` у GRPO-моделей  ~0.7-1.6% — даже когда модель даёт правильный финальный ответ, промежуточные шаги почти никогда не верны.

Модель после RL научилась выдавать правильный ответ, полностью игнорируя свою цепочку ризонинга. Шаги выглядят как ризонинг (правильный формат, символы операторов), но числа в них рандом. (или я неправильно измерил, что тоже вероятно)


**Что случилось??** Ревард в трейне скорее наверное был настроен с маленьким `step_weight` (в плане что за правильные степы был маленький ревард), поэтому модель нашла шорткат: юзать паттерн "формат шагов + ответ" без реального вычисления. На easy задачах этого хватает (операторы простые, ответ можно угадать), на hard — нет.

**Как фиксить?**

- Наверное можно попробовать зашедулить куррикулум - сначала давать простые задачи, потом посложнее 
- Может убрать формат на <thinking>.
- Поднять  step_weight


В целом очень крутая домашка, удалось поиграться, даже еще покрутить захотелось, пока нормально модель не обучится, посмотрим что выйдет. 